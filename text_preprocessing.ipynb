{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "주어진 데이터를 토큰이라 불리는 단위로 나누는 작업으로 토큰이 되는 기준은 설정하기에 따라 어절, 단어, 형태소, 음절, 자소 등 다를 수 있다. 일반적으로 Character-based Tokenization / Word-based Tokenization / Subword-based Tokenization으로 구분된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LegendKi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nltk에서는 여러 tokenizer들을 제공하며 라이브러리에 있는 함수들을 사용해 사전에 정의된 tokenizer의 규칙에 따라 쉽게 자연어 문장을 토큰화 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"You won't be able to know about natural language perfectly, but you need to master a number of preprocessing skills to perform natural language processing.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word_tokenize\n",
    "- nltk의 word_tokenize는 문장을 단어 기반으로 토큰화하며 space와 구두 점을 기준으로 토큰화 한다. \n",
    "- 해당 함수는 won't를 wo / n't로 분리 하는 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You',\n",
       " 'wo',\n",
       " \"n't\",\n",
       " 'be',\n",
       " 'able',\n",
       " 'to',\n",
       " 'know',\n",
       " 'about',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'perfectly',\n",
       " ',',\n",
       " 'but',\n",
       " 'you',\n",
       " 'need',\n",
       " 'to',\n",
       " 'master',\n",
       " 'a',\n",
       " 'number',\n",
       " 'of',\n",
       " 'preprocessing',\n",
       " 'skills',\n",
       " 'to',\n",
       " 'perform',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordPunctTokenizer\n",
    "- WordPunctTokenizer는 word_tokenize에 대한 대안으로써 구두점을 별도로 분류하는 특징을 가지고 있기 때문에 word_tokenize와 달리 won't를 won과 t로 분리한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You',\n",
       " 'won',\n",
       " \"'\",\n",
       " 't',\n",
       " 'be',\n",
       " 'able',\n",
       " 'to',\n",
       " 'know',\n",
       " 'about',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'perfectly',\n",
       " ',',\n",
       " 'but',\n",
       " 'you',\n",
       " 'need',\n",
       " 'to',\n",
       " 'master',\n",
       " 'a',\n",
       " 'number',\n",
       " 'of',\n",
       " 'preprocessing',\n",
       " 'skills',\n",
       " 'to',\n",
       " 'perform',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punct_tokenizer = WordPunctTokenizer()\n",
    "\n",
    "punct_tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TreebankWordTokenizer\n",
    "- TreebankWordTokenizer는 가장 표준적인 방법의 토큰화 방법으로 사용되는 tokenizer이다.\n",
    "- TreebankWordTokenizer는 두가지 규칙을 갖는다.\n",
    "    - 1. 하이폰으로 구성된 단어는 하나로 유지한다.\n",
    "    - 2. dosen't와 같이 어퍼스트로피로 접어가 함께하는 단어는 분리해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You',\n",
       " 'wo',\n",
       " \"n't\",\n",
       " 'be',\n",
       " 'able',\n",
       " 'to',\n",
       " 'know',\n",
       " 'about',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'perfectly',\n",
       " ',',\n",
       " 'but',\n",
       " 'you',\n",
       " 'need',\n",
       " 'to',\n",
       " 'master',\n",
       " 'a',\n",
       " 'number',\n",
       " 'of',\n",
       " 'preprocessing',\n",
       " 'skills',\n",
       " 'to',\n",
       " 'perform',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "tree_tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문장 토큰화\n",
    "- nltk.sent_tokenize를 사용하면 마침표에 따라 여러 문장들을 문장 단위로 구분할 수 있다.\n",
    "- sent_tokenize는 단순히 마침표의 유무 만으로 문장의 종결여부를 판단하지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = \"NLP stands for Natural Language Processing. \\\n",
    "It is a branch of artificial intelligence (AI) that focuses on the interaction between computers and human language. \\\n",
    "NLP combines techniques from linguistics, computer science, and machine learning to enable computers to understand, interpret, \\\n",
    "and generate human language in a way that is meaningful and useful.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLP stands for Natural Language Processing.',\n",
       " 'It is a branch of artificial intelligence (AI) that focuses on the interaction between computers and human language.',\n",
       " 'NLP combines techniques from linguistics, computer science, and machine learning to enable computers to understand, interpret, and generate human language in a way that is meaningful and useful.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.sent_tokenize(sentences) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = \"I am actively looking for Ph.D. students. and you are a Ph.D student.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am actively looking for Ph.D. students.', 'and you are a Ph.D student.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.sent_tokenize(sentences) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning\n",
    "코퍼스 내에서 tokenize 작업에 방해가 되거나 의미가 없는 부분의 텍스트, 노이즈를 제거하는 작업으로 tokenize 전에 정제를 하기도 하지만 이후에도 여전히 남아있는 노이즈들을 제거하기 위해 지속적으로 수행한다. 이 때 노이즈는 특수 문자같은 아무 의미를 갖지않는 글자들을 의미하기도 하지만 분석하고 하는 목적에 맞지 않는 불필요한 단어들을 의미한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LegendKi\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'itself', 'no', 'is', 'by', 'above', 'o', 've', \"you'll\", \"you're\", 'after', 'only', 'didn', 'how', 'ain', \"aren't\", 'into', 'in', \"doesn't\", 'yourselves', 'was', 'they', 'own', 'mightn', 'our', 'but', \"should've\", 't', 'his', 'under', 'once', \"hasn't\", 'both', 'so', 'theirs', 'weren', 'too', 'any', 'should', 'shouldn', 'doesn', 'and', 'haven', \"weren't\", 'hadn', 'hers', 'he', 'these', 'hasn', 'most', 'of', 'can', 'during', 'couldn', 'needn', \"she's\", 'while', 'each', 'if', \"isn't\", 'between', 'isn', 'him', \"shouldn't\", 'against', 'why', \"hadn't\", 'y', 'myself', 'you', 'here', 'where', 'been', 'your', 'for', 'she', 'down', 'are', 'ma', 'me', 'yours', 'am', 'a', 'wasn', 're', 'not', \"couldn't\", \"haven't\", \"you'd\", 'before', 'all', 'to', 'their', \"needn't\", \"wouldn't\", 'off', 'when', 'same', 'with', 'other', 'aren', 'its', 'were', 'doing', 'the', 'whom', \"you've\", 'than', 'from', \"didn't\", \"wasn't\", 'himself', 'do', \"shan't\", 'that', 'd', 'my', 'ourselves', 's', 'such', 'now', 'themselves', 'out', 'about', 'just', 'as', 'mustn', 'her', 'or', 'an', \"don't\", 'ours', 'below', 'nor', 'very', 'there', \"that'll\", 'on', 'through', 'won', 'then', 'who', 'them', 'did', 'we', 'those', 'up', 'm', \"mightn't\", 'being', 'more', 'i', 'because', 'it', \"it's\", 'what', 'don', 'have', 'further', 'does', 'yourself', 'few', \"mustn't\", 'had', 'this', 'will', 'over', 'at', 'shan', 'again', 'll', 'which', 'until', 'herself', 'some', 'having', 'has', 'wouldn', 'be', \"won't\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords') \n",
    "\n",
    "stop=set(stopwords.words('english'))\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nltk에서는 사전에 정의된 세트를 갖고 있으며 쉽게 로드하여 사용할 수 있따. 이를 활용해 불용어를 제외한 단어들을 쉽게 추출한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 포함:  ['I', 'want', 'to', 'go', 'to', 'shopping', 'and', 'a', 'I', 'want', 'to', 'buy', 'some', 'of', 'snack']\n",
      "불용어 미포함:  ['want', 'go', 'shopping', 'want', 'buy', 'snack']\n"
     ]
    }
   ],
   "source": [
    "sen=\"I want to go to shopping and a I want to buy some of snack\"\n",
    "tokens=nltk.word_tokenize(sen)\n",
    "\n",
    "clean_tokens=[]\n",
    "for tok in tokens:\n",
    "  if len(tok.lower())>1 and (tok.lower() not in stop): \n",
    "    clean_tokens.append(tok)\n",
    "\n",
    "\n",
    "print(\"불용어 포함: \",tokens)\n",
    "print(\"불용어 미포함: \",clean_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "Normalization은 의미가 중복되거나 의미론적으로 유사한 단어들을 하나로 통합하거나 단어의 원형을 찾아 통일해주는 작업으로 크게 Stemming과 Lemmatization이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "- stem : 줄기라는 뜻을 가진 영어단어로 언어학에서는 stem을 어간이라고 한다.\n",
    "- 어간은 굴절하는 단어에서 변화하지 않는 부분을 의미하며 stemming이란 어간 추출을 말한다.\n",
    "- 예를 들어 going이라는 단어가 있다면 Stemming을 진행할 시 go가 된다. Computers라는 단어를 Stemming을 진행할 시 Comput를 추출하는 과정을 Stemming이라고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n"
     ]
    }
   ],
   "source": [
    "s = PorterStemmer() \n",
    "text = \"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\n",
    "words = word_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This ==> thi\n",
      "was ==> wa\n",
      "not ==> not\n",
      "the ==> the\n",
      "map ==> map\n",
      "we ==> we\n",
      "found ==> found\n",
      "in ==> in\n",
      "Billy ==> billi\n",
      "Bones ==> bone\n",
      "'s ==> 's\n",
      "chest ==> chest\n",
      ", ==> ,\n",
      "but ==> but\n",
      "an ==> an\n",
      "accurate ==> accur\n",
      "copy ==> copi\n",
      ", ==> ,\n",
      "complete ==> complet\n",
      "in ==> in\n",
      "all ==> all\n",
      "things ==> thing\n",
      "-- ==> --\n",
      "names ==> name\n",
      "and ==> and\n",
      "heights ==> height\n",
      "and ==> and\n",
      "soundings ==> sound\n",
      "-- ==> --\n",
      "with ==> with\n",
      "the ==> the\n",
      "single ==> singl\n",
      "exception ==> except\n",
      "of ==> of\n",
      "the ==> the\n",
      "red ==> red\n",
      "crosses ==> cross\n",
      "and ==> and\n",
      "the ==> the\n",
      "written ==> written\n",
      "notes ==> note\n",
      ". ==> .\n"
     ]
    }
   ],
   "source": [
    "for i in words:\n",
    "  print(f'{i} ==> {s.stem(i)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "- Lemmatization이란 문장 속에서 다양한 형태롤 활용된 단어의 표제어를 찾는 일을 뜻한다.\n",
    "- 여기서 말하는 표제어란 사전에서 단어의 뜻을 찾을 때 쓰는 기본형으로 Lemmatization은 단어의 원형을 추출해주는 역할을 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\LegendKi\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "words = ['doing', 'has', 'going', 'loves', 'lives', 'flying', 'dies', 'watching', 'started', 'seen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing ==> do\n",
      "has ==> have\n",
      "going ==> go\n",
      "loves ==> love\n",
      "lives ==> live\n",
      "flying ==> fly\n",
      "dies ==> die\n",
      "watching ==> watch\n",
      "started ==> start\n",
      "seen ==> see\n"
     ]
    }
   ],
   "source": [
    "for i in words:\n",
    "  lemma = lemmatizer.lemmatize(i, pos='v')\n",
    "  print(f'{i} ==> {lemma}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edit Distance\n",
    "Edit Distance란 2개의 문자열이 얼만큼 다른가를 거리개념으로 치환해 숫자로 표현한 것으로 Edit Distance에는 삽입, 삭제, 교체로 크게 3가지 연산이 존재한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.metrics import edit_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(edit_distance(\"CAT\",\"HAT\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
